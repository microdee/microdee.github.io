## Brainstorming SG-RDW

**Signature Graph of Records and Derivative Works**

[(05.04.2022)](/c/log/sg-rdw)

### The problem

Mass communication of events and phenomena in 2010's and the rest of 2020's is characterized by the intentionally introduced noise into the circulation of information by fraudelant actors with their own individual agendas to justify that. This is possible by the ever increasing number of users/organizations who can reach wider audiences without significant initial investments of time and resources. The mentioned audience then decides whether those outlets matches their preferred narrative and they build their trust towards those outlets solely based on their preference. Before I go on I personally find this kind of democratization a good thing, especially when it comes to cultural/artistic topics, fiction, hobbies etc.

But when it comes to topics which needs to model reality in precise ways, like scientific claims, physical-mental health and news, that narrative-based trust can help internalize unrealistic world-views which might put others in unnecessary avoidable danger. Currently the average informational content consumer has no immediate trustless guarantee of the correctness of the consumed content **\[look it up tho\]**. They may cross-check with other sources but that means they either find the same information twice or more times with again no trustless guarantees, or find content contradicting each-other making the subject of those ambiguous for the consumer. In both cases there are no unified ways to check how the content was put together, what were their sources and what parts they derived from those sources. Unless of course the authors explicitly provide such info, but that may still be subject to arbitrary curation to support the agenda of said authors.

There are of course initiatives to fix that **\[list initiatives\]** but most is based on analysing the content itself with a wide variety of machine-learning methods, and tell the consumer that given content *LOOKS* legit, or it *LOOKS* incorrect. However those methods rely on the appearance and structure of a content, maybe they can even cross-check with other public content, but their precision is as good as the balance of their training process. Therefore, using machine-learning based methods just shifts the trust from the content author, to the developer's training data of the content analyzer.

What I'd propose is a technological/cultural workflow...

**TL;DR**: Incorrect (or if you prefer Fake) news which doesn't match actual reality is causing harm for millions today, and this brainstorming would propose a simple technological remedy against that.